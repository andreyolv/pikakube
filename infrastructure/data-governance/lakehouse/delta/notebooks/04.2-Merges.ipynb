{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a3e5f-3c7a-4aff-8950-787caa42ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"DeltaSession\") \\\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af692f3-5876-4025-9732-82958cad4035",
   "metadata": {},
   "source": [
    "## [whenMatchedUpdate](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenMatchedUpdate)\n",
    "\n",
    "whenMatchedUpdate(condition: Union[pyspark.sql.column.Column, str, None] = None, set: Optional[Dict[str, Union[str, pyspark.sql.column.Column]]] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Update a matched table row based on the rules defined by set. If a condition is specified, then it must evaluate to true for the row to be updated.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the update\n",
    "- set (dict with str as keys and str or pyspark.sql.Column as values) – Defines the rules of setting the values of columns that need to be updated. Note: This param is required. Default value None is present to allow positional args in same order across languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c0cac-e532-4af5-8b99-8900da369b52",
   "metadata": {},
   "source": [
    "## [whenMatchedUpdateAll](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenMatchedUpdateAll)\n",
    "\n",
    "whenMatchedUpdateAll(condition: Union[pyspark.sql.column.Column, str, None] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Update all the columns of the matched table row with the values of the corresponding columns in the source row. If a condition is specified, then it must be true for the new row to be updated.\n",
    "\n",
    "See DeltaMergeBuilder for complete usage details.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the insert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90945b-f5cb-428f-8740-98cbf4289efa",
   "metadata": {},
   "source": [
    "## [whenMatchedDelete](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenMatchedDelete)\n",
    "\n",
    "whenMatchedDelete(condition: Union[pyspark.sql.column.Column, str, None] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Delete a matched row from the table only if the given condition (if specified) is true for the matched row.\n",
    "\n",
    "See DeltaMergeBuilder for complete usage details.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3237c0-5976-4cb1-932a-5ed504db1de1",
   "metadata": {},
   "source": [
    "## [whenNotMatchedInsert](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenNotMatchedInsert)\n",
    "\n",
    "whenNotMatchedInsert(condition: Union[pyspark.sql.column.Column, str, None] = None, values: Optional[Dict[str, Union[str, pyspark.sql.column.Column]]] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Insert a new row to the target table based on the rules defined by values. If a condition is specified, then it must evaluate to true for the new row to be inserted.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the insert\n",
    "- values (dict with str as keys and str or pyspark.sql.Column as values) – Defines the rules of setting the values of columns that need to be updated. Note: This param is required. Default value None is present to allow positional args in same order across languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5f693-37c3-4f42-9f85-82aa513230b8",
   "metadata": {},
   "source": [
    "## [whenNotMatchedInsertAll](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenNotMatchedInsertAll)\n",
    "\n",
    "whenNotMatchedInsertAll(condition: Union[pyspark.sql.column.Column, str, None] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Insert a new target Delta table row by assigning the target columns to the values of the corresponding columns in the source row. If a condition is specified, then it must evaluate to true for the new row to be inserted.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the insert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad656c-e5de-48a6-8ba7-567cc3aeab31",
   "metadata": {},
   "source": [
    "## whenNotMatchedInsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3909d7-5f28-48c9-ab48-0265feed55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(0, \"Bob\", 23), \n",
    "        (1, \"Sue\", 25), \n",
    "        (2, \"Jim\", 27)]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"id\", \"name\", \"age\")\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c5fdb-08a3-4e4e-898a-65501741040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.format(\"delta\").save(\"extract/04merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9aecc3-9740-4dba-8ec4-b0d1b6bc4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    (0, \"Bob\", 23),  # exists in our original dataset above\n",
    "    (3, \"Sally\", 30),  # new data\n",
    "    (4, \"Henry\", 33),  # new data\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data).toDF(\"id\", \"name\", \"age\").repartition(1)\n",
    "\n",
    "new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99352554-243c-42e2-b4af-c7c286e210c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table = DeltaTable.forPath(spark, \"extract/04merge\")\n",
    "\n",
    "people_table.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a16769-0ea6-4f69-909c-79386306b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    people_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\"id\": \"source.id\", \n",
    "                \"name\": \"source.name\"}\n",
    "    ).execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f032b-4c8a-4718-8d49-bd6a983a73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table.toDF().limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e502302-7aa1-4455-a31a-7313bb70fbbf",
   "metadata": {},
   "source": [
    "## whenNotMatchedInsertAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe0e49-8959-4781-bf0b-51025f777376",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    (0, \"Bob\", 23),  # exists in our original dataset above\n",
    "    (5, \"Thamires\", 30),  # new data\n",
    "    (6, \"Cristian\", 33),  # new data\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data).toDF(\"id\", \"name\", \"age\").repartition(1)\n",
    "\n",
    "new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef56f1-a7ab-4506-aeb4-ee1991734963",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    people_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    ).whenNotMatchedInsertAll(\n",
    "    ).execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496c1ab-b9a6-4dd0-a62e-874ebece51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table.toDF().orderBy('id').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06107beb-bafc-4919-ba36-7bb1082b1958",
   "metadata": {},
   "source": [
    "## whenMatchedUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5263ac-dea1-414f-893c-e3daa3478cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    (4, \"Henry\", 34),\n",
    "    (10, \"Allie\", 22),\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data).toDF(\"id\", \"name\", \"age\").repartition(1)\n",
    "\n",
    "new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd8f3f-bd43-402f-8873-587a3c1e8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    people_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\"age\": \"source.age\"}\n",
    "    )\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a43e27-a139-46e1-89e4-eff01820b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table.toDF().limit(10).orderBy('id').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7feadf-e3a9-46ad-8205-6a8b20999616",
   "metadata": {},
   "source": [
    "## Apply change data with merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc127b98-70f1-4568-b5c7-23c5fb3e32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    (9, \"Richard\", 75, \"INSERT\"),\n",
    "    (3, \"Sally\", 31, \"UPDATE\"),\n",
    "    (0, \"Bob\", 23, \"DELETE\"),\n",
    "]\n",
    "\n",
    "\n",
    "new_df = spark.createDataFrame(new_data).toDF(\"id\", \"name\", \"age\", \"_op\").repartition(1)\n",
    "\n",
    "new_df.orderBy('id').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7738f5-d55b-4ab3-a6af-3ee840d61414",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    people_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    ).whenNotMatchedInsert(\n",
    "        condition='source._op = \"INSERT\"',\n",
    "        values={\"id\": \"source.id\", \n",
    "                \"name\": \"source.name\", \n",
    "                \"age\": \"source.age\"},\n",
    "    ).whenMatchedUpdate(\n",
    "        condition='source._op = \"UPDATE\"',\n",
    "        set={\"id\": \"source.id\", \n",
    "             \"name\": \"source.name\", \n",
    "             \"age\": \"source.age\"},\n",
    "    ).whenMatchedDelete(\n",
    "        condition='source._op = \"DELETE\"'\n",
    "    ).execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4b0e5-eda7-469e-9ab6-3e643cb26d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table.toDF().limit(10).orderBy('id').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a5978b-47f7-4387-9537-7b5636d36adb",
   "metadata": {},
   "source": [
    "## Delta Lake merge for partial Change Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d4ff0-9970-4e84-8ac3-ffab37f0bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    (1, \"SueNew\", None, \"UPDATE\"),\n",
    "    (3, None, 32, \"UPDATE\"),\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data).toDF(\"id\", \"name\", \"age\", \"_op\").repartition(1)\n",
    "\n",
    "new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2cf56c-d57e-49bf-915e-8e5f0a4e3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    people_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition='source._op = \"UPDATE\"',\n",
    "        set={\n",
    "            \"id\": \"source.id\",\n",
    "            \"name\": \"CASE WHEN source.name IS NOT NULL THEN source.name ELSE target.name END\",\n",
    "            \"age\": \"CASE WHEN source.age IS NOT NULL THEN source.age ELSE target.age END\",\n",
    "        },\n",
    "    ).execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e965c02-3c26-4884-9867-bd80b821ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table.toDF().orderBy('id').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f0f60-9035-4555-86f2-2b906ca13577",
   "metadata": {},
   "source": [
    "## [whenNotMatchedBySourceUpdate](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenNotMatchedBySourceUpdate)\n",
    "\n",
    "whenNotMatchedBySourceUpdate(condition: Union[pyspark.sql.column.Column, str, None] = None, set: Optional[Dict[str, Union[str, pyspark.sql.column.Column]]] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Update a target row that has no matches in the source based on the rules defined by set. If a condition is specified, then it must evaluate to true for the row to be updated.\n",
    "\n",
    "See DeltaMergeBuilder for complete usage details.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the update\n",
    "- set (dict with str as keys and str or pyspark.sql.Column as values) – Defines the rules of setting the values of columns that need to be updated. Note: This param is required. Default value None is present to allow positional args in same order across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e56bc-ea26-422c-87ab-89990a770192",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (0, \"Bob\", 23, datetime.date(2022, 1, 2), \"inactive\"),  # inactive\n",
    "    (1, \"Sue\", 25, datetime.date(2023, 4, 5), \"active\"),  # active\n",
    "    # marked as active, but should not be active anymore\n",
    "    (2, \"Jim\", 27, datetime.date(2023, 2, 7), \"active\",),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"id\", \"name\", \"age\", \"last_seen\", \"status\")\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9e0b7-443c-4065-825d-8f01861bda5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.format(\"delta\").save(\"extract/04bysource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd63369-23bd-4166-a62f-0bd644f320b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table = DeltaTable.forPath(spark, \"extract/04bysource\")\n",
    "\n",
    "customers_table.toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9342cf-f5ac-4112-813b-443781c459f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    (0, \"Bob\", 23, datetime.date.today()),  # existing customer\n",
    "    (3, \"Sally\", 30, datetime.date.today()),  # new customer\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data).toDF(\"id\", \"name\", \"age\", \"current_date\")\n",
    "\n",
    "new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6547a5-1daf-4e15-a821-9a47f5c53e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    customers_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        set={\"target.last_seen\": \"source.current_date\", \n",
    "             \"target.status\": \"'active'\"}\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"target.id\": \"source.id\",\n",
    "            \"target.name\": \"source.name\",\n",
    "            \"target.age\": \"source.age\",\n",
    "            \"target.last_seen\": \"source.current_date\",\n",
    "            \"target.status\": \"'active'\",\n",
    "        }\n",
    "    )\n",
    "    .whenNotMatchedBySourceUpdate(\n",
    "        condition=\"target.last_seen <= (current_date() - INTERVAL '30' DAY)\",\n",
    "        set={\"target.status\": \"'inactive'\"},\n",
    "    )\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa48e10-22bd-4eff-a4c6-3d5b3dea6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forPath(spark, \"extract/04bysource\").toDF().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34f9eb-203e-42df-a4db-19a32cbe93dd",
   "metadata": {},
   "source": [
    "## [whenNotMatchedBySourceDelete](https://docs.delta.io/latest/api/python/index.html#delta.tables.DeltaMergeBuilder.whenNotMatchedBySourceDelete)\n",
    "\n",
    "whenNotMatchedBySourceDelete(condition: Union[pyspark.sql.column.Column, str, None] = None) → delta.tables.DeltaMergeBuilder\n",
    "\n",
    "Delete a target row that has no matches in the source from the table only if the given condition (if specified) is true for the target row.\n",
    "\n",
    "Parameters:\n",
    "- condition (str or pyspark.sql.Column) – Optional condition of the delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f7c62-39d3-463f-a0f7-275a6a61da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    customers_table.alias(\"target\")\n",
    "    .merge(\n",
    "        source=new_df.alias(\"source\"), \n",
    "        condition=\"target.id = source.id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        set={\"target.last_seen\": \"source.current_date\", \n",
    "             \"target.status\": \"'active'\"}\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"target.id\": \"source.id\",\n",
    "            \"target.name\": \"source.name\",\n",
    "            \"target.age\": \"source.age\",\n",
    "            \"target.last_seen\": \"source.current_date\",\n",
    "            \"target.status\": \"'active'\",\n",
    "        }\n",
    "    )\n",
    "    .whenNotMatchedBySourceDelete(\n",
    "        condition=\"target.last_seen <= (current_date() - INTERVAL '30' DAY)\",\n",
    "    )\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57e5bb-7108-4025-9b0d-50f6efd850d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forPath(spark, \"extract/04bysource\").toDF().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d8cff-d2bd-47ad-adbb-c882cc6b0f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
