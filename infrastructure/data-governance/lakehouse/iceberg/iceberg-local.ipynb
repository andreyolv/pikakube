{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fba0703-3ea2-4c79-92a7-6cf4acdc29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "catalog_name = 'testdb' # 'local'\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"IcebergSession\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.2.1\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", \"/home/jovyan/warehouse2\")\n",
    "        .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52ac464-fe8e-428c-bc89-1422fb0aa9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_table_iceberg = spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS local.db.example \n",
    "        (\n",
    "           id int, \n",
    "           name string,\n",
    "           age int,\n",
    "           city string,\n",
    "           __op string\n",
    "        ) \n",
    "        USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ce96ef8-e9eb-49c5-a8cc-e628d2b74c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-------------+----+\n",
      "| id|  name|age|         city|__op|\n",
      "+---+------+---+-------------+----+\n",
      "|  1| Alice| 25|     New York|   r|\n",
      "|  2|  Joao| 30|San Francisco|   r|\n",
      "|  3|Flavio| 21|    Sao Paulo|   r|\n",
      "+---+------+---+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for a DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"__op\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "data = [(1, \"Alice\", 25, \"New York\", \"r\"), \n",
    "        (2, \"Joao\", 30, \"San Francisco\", \"r\"), \n",
    "        (3, \"Flavio\", 21, \"Sao Paulo\", \"r\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6520e3df-af2b-4695-8aa1-1f5296587e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    "     .format(\"iceberg\")\n",
    "     .mode(\"overwrite\")\n",
    "     .save(\"local.db.example\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2914c6-b471-4651-b40c-0357b160dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-------------+----+\n",
      "| id|  name|age|         city|__op|\n",
      "+---+------+---+-------------+----+\n",
      "|  1| Alice| 25|     New York|   r|\n",
      "|  2|  Joao| 30|San Francisco|   r|\n",
      "|  3|Flavio| 21|    Sao Paulo|   r|\n",
      "+---+------+---+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"iceberg\").load(\"local.db.example\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9d5ed-24b5-4088-8c78-fd685da97aa6",
   "metadata": {},
   "source": [
    "## Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3942ecb9-a89a-44ac-bc24-4e2a15f50d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for a DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"__op\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "data = [(4, \"Alic2e\", 252, \"New York2\", \"r\"), \n",
    "        (5, \"Flavio2\", 212, \"Sao Paulo2\", \"r\")]\n",
    "\n",
    "df_new = spark.createDataFrame(data, schema)\n",
    "\n",
    "(df_new.write\n",
    "     .format(\"iceberg\")\n",
    "     .mode(\"append\")\n",
    "     .save(\"local.db.example\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0da8b26b-40c2-4b1c-ac9a-e02e05bc983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-04-26 22:17:...|6372009097357199076|               null|               true|\n",
      "|2024-04-26 22:28:...|4004617722887816301|6372009097357199076|               true|\n",
      "|2024-04-26 22:29:...|5616672445895419694|4004617722887816301|               true|\n",
      "|2024-04-26 22:29:...|3648637280184859472|5616672445895419694|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM local.db.example.history;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3989e3d8-e642-4d3b-b66d-c822385b2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-------------+----+\n",
      "| id|   name|age|         city|__op|\n",
      "+---+-------+---+-------------+----+\n",
      "|  4| Alic2e|252|    New York2|   r|\n",
      "|  5|Flavio2|212|   Sao Paulo2|   r|\n",
      "|  1|  Alice| 25|     New York|   r|\n",
      "|  2|   Joao| 30|San Francisco|   r|\n",
      "|  3| Flavio| 21|    Sao Paulo|   r|\n",
      "+---+-------+---+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM local.db.example;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d820859-4464-4505-9466-bf27c07566aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-------------+----+\n",
      "| id|  name|age|         city|__op|\n",
      "+---+------+---+-------------+----+\n",
      "|  1| Alice| 25|     New York|   r|\n",
      "|  2|  Joao| 30|San Francisco|   r|\n",
      "|  3|Flavio| 21|    Sao Paulo|   r|\n",
      "+---+------+---+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM local.db.example FOR SYSTEM_VERSION AS OF 6372009097357199076;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad6fe829-49d2-4081-a230-5220e94ac315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+----------+----+\n",
      "| id|   name|age|      city|__op|\n",
      "+---+-------+---+----------+----+\n",
      "|  4| Alic2e|252| New York2|   r|\n",
      "|  5|Flavio2|212|Sao Paulo2|   r|\n",
      "+---+-------+---+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM local.db.example FOR SYSTEM_VERSION AS OF 4004617722887816301;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b2194c6-39de-4d46-8d03-8313c2dd55f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+-------------+----+\n",
      "| id|  name|age|         city|__op|\n",
      "+---+------+---+-------------+----+\n",
      "|  1| Alice| 25|     New York|   r|\n",
      "|  2|  Joao| 30|San Francisco|   r|\n",
      "|  3|Flavio| 21|    Sao Paulo|   r|\n",
      "+---+------+---+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").option(\"snapshot-id\", 6372009097357199076).load(\"local.db.example\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a04d25e1-2ead-4f22-8088-7ff89dadea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-04-26 22:17:...|6372009097357199076|               null|overwrite|/home/jovyan/ware...|{spark.app.id -> ...|\n",
      "|2024-04-26 22:28:...|4004617722887816301|6372009097357199076|overwrite|/home/jovyan/ware...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM local.db.example.snapshots;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26170cc5-a99f-4a2b-9db4-8ea40e2ebab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|           timestamp|                file| latest_snapshot_id|latest_schema_id|latest_sequence_number|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|2024-04-26 22:11:...|/home/jovyan/ware...|               null|            null|                  null|\n",
      "|2024-04-26 22:17:...|/home/jovyan/ware...|6372009097357199076|               0|                     0|\n",
      "|2024-04-26 22:28:...|/home/jovyan/ware...|4004617722887816301|               0|                     0|\n",
      "|2024-04-26 22:29:...|/home/jovyan/ware...|5616672445895419694|               0|                     0|\n",
      "|2024-04-26 22:29:...|/home/jovyan/ware...|3648637280184859472|               0|                     0|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM local.db.example.metadata_log_entries;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799028c8-3078-4023-8f14-7790e7e0057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for a DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"__op\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the schema\n",
    "data = [(1, \"Alice\", 25, \"New York\", \"r\"), \n",
    "        (2, \"Joao\", 30, \"San Francisco\", \"r\"), \n",
    "        (3, \"Flavio\", 21, \"Sao Paulo\", \"r\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "(df.write\n",
    " .format(\"iceberg\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(f\"{catalog_name}.db2.tabela_iceberg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca733c52-cb45-4e1a-b6c2-e63bcdd3e766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- __op: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 37552)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"iceberg\").load(f\"{catalog_name}.db2.tabela_iceberg\")\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065aa1b5-7631-4674-8656-891198fcfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming https://iceberg.apache.org/docs/latest/spark-structured-streaming/\n",
    "# Update https://iceberg.apache.org/docs/latest/spark-writes/\n",
    "# Spark DDL https://iceberg.apache.org/docs/latest/spark-ddl/\n",
    "# Branching and Tagging https://iceberg.apache.org/docs/latest/branching/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
