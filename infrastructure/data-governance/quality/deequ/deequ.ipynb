{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564a6c7a-a5b7-4613-97a6-5441c51838f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pydeequ\n",
      "  Downloading pydeequ-1.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/site-packages (from pydeequ) (1.23.3)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.10/site-packages (from pydeequ) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=0.23.0->pydeequ) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.23.0->pydeequ) (1.16.0)\n",
      "Downloading pydeequ-1.3.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pydeequ\n",
      "Successfully installed pydeequ-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a8bc9-e15d-4ba2-840d-486c123ec3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/data-hackers/qualidade-de-dados-na-pr%C3%A1tica-com-spark-e-aws-deequ-ec8127979ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb61a1d4-3e04-450e-9ffe-52871988b022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "os.environ['SPARK_VERSION'] = str(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f07d5ce-2d4f-4ae7-b87f-7e15b06efe62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3c04d3-ab7d-4530-8dda-4ed1188d618b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/coder/.ivy2/cache\n",
      "The jars for the packages stored in: /home/coder/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-565d427d-7421-4fe2-9607-4257aa017d7d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.7-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.7-spark-3.3/deequ-2.0.7-spark-3.3.jar ...\n",
      "\t[SUCCESSFUL ] com.amazon.deequ#deequ;2.0.7-spark-3.3!deequ.jar (211ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.10!scala-reflect.jar (89ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.12/0.13.2/breeze_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze_2.12;0.13.2!breeze_2.12.jar (164ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.12/0.13.2/breeze-macros_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze-macros_2.12;0.13.2!breeze-macros_2.12.jar (50ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (50ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (55ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.2!commons-math3.jar (60ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire_2.12/0.13.0/spire_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire_2.12;0.13.0!spire_2.12.jar (127ms)\n",
      "downloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.2/shapeless_2.12-2.3.2.jar ...\n",
      "\t[SUCCESSFUL ] com.chuusai#shapeless_2.12;2.3.2!shapeless_2.12.jar(bundle) (90ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.8.2!junit.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.12/0.13.0/spire-macros_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire-macros_2.12;0.13.0!spire-macros_2.12.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.12/0.6.1/machinist_2.12-0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#machinist_2.12;0.6.1!machinist_2.12.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#macro-compat_2.12;1.1.1!macro-compat_2.12.jar (47ms)\n",
      ":: resolution report :: resolve 5267ms :: artifacts dl 1191ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.7-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   15  |   15  |   2   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-565d427d-7421-4fe2-9607-4257aa017d7d\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (33363kB/33ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/01 13:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|   c|\n",
      "+---+---+----+\n",
      "|foo|  1|   5|\n",
      "|bar|  2|   6|\n",
      "|baz|  3|null|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .appName(\"Exemplo DataFrame PySpark\")\n",
    "            .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) # https://mvnrepository.com/artifact/com.amazon.deequ/deequ\n",
    "            .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "dados = [\n",
    "    Row(a=\"foo\", b=1, c=5),\n",
    "    Row(a=\"bar\", b=2, c=6),\n",
    "    Row(a=\"baz\", b=3, c=None)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(dados)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d81e9-6c53-4c17-bf35-b65343260e5e",
   "metadata": {},
   "source": [
    "## Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a563952-602e-465d-b3f1-9b64ae87f18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+-----+\n",
      "| entity|instance|        name|value|\n",
      "+-------+--------+------------+-----+\n",
      "|Dataset|       *|        Size|  3.0|\n",
      "| Column|       b|Completeness|  1.0|\n",
      "+-------+--------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"b\")) \\\n",
    "                    .run()\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263cb90-3e67-4f12-87a3-fdd1a7cccba8",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec20e65b-10ec-41f5-861d-a0c27dec6b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/01 14:03:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardProfiles for column: a: {\n",
      "    \"completeness\": 1.0,\n",
      "    \"approximateNumDistinctValues\": 3,\n",
      "    \"dataType\": \"String\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {\n",
      "        \"Boolean\": 0,\n",
      "        \"Fractional\": 0,\n",
      "        \"Integral\": 0,\n",
      "        \"Unknown\": 0,\n",
      "        \"String\": 3\n",
      "    },\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"baz\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"foo\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"bar\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ]\n",
      "    ]\n",
      "}\n",
      "NumericProfiles for column: b: {\n",
      "    \"completeness\": 1.0,\n",
      "    \"approximateNumDistinctValues\": 3,\n",
      "    \"dataType\": \"Integral\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {},\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"1\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"2\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"3\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ]\n",
      "    ],\n",
      "    \"kll\": \"None\",\n",
      "    \"mean\": 2.0,\n",
      "    \"maximum\": 3.0,\n",
      "    \"minimum\": 1.0,\n",
      "    \"sum\": 6.0,\n",
      "    \"stdDev\": 0.816496580927726,\n",
      "    \"approxPercentiles\": []\n",
      "}\n",
      "NumericProfiles for column: c: {\n",
      "    \"completeness\": 0.6666666666666666,\n",
      "    \"approximateNumDistinctValues\": 2,\n",
      "    \"dataType\": \"Integral\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {},\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"5\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"NullValue\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"6\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ]\n",
      "    ],\n",
      "    \"kll\": \"None\",\n",
      "    \"mean\": 5.5,\n",
      "    \"maximum\": 6.0,\n",
      "    \"minimum\": 5.0,\n",
      "    \"sum\": 11.0,\n",
      "    \"stdDev\": 0.5,\n",
      "    \"approxPercentiles\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.profiles import *\n",
    "\n",
    "result = ColumnProfilerRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .run()\n",
    "\n",
    "for col, profile in result.profiles.items():\n",
    "    print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4c7dc-04ff-4a5e-b75e-f02fc9d6713c",
   "metadata": {},
   "source": [
    "## Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f93f6a-f762-45d7-a4c3-f4a77806bff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'constraint_suggestions': [{'constraint_name': 'CompletenessConstraint(Completeness(a,None,None))', 'column_name': 'a', 'current_value': 'Completeness: 1.0', 'description': \"'a' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"a\")'}, {'constraint_name': 'UniquenessConstraint(Uniqueness(List(a),None,None))', 'column_name': 'a', 'current_value': 'ApproxDistinctness: 1.0', 'description': \"'a' is unique\", 'suggesting_rule': 'UniqueIfApproximatelyUniqueRule()', 'rule_description': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint', 'code_for_constraint': '.isUnique(\"a\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(b,None,None))', 'column_name': 'b', 'current_value': 'Completeness: 1.0', 'description': \"'b' is not null\", 'suggesting_rule': 'CompleteIfCompleteRule()', 'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint', 'code_for_constraint': '.isComplete(\"b\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('b' has no negative values,b >= 0,None,List(b),None))\", 'column_name': 'b', 'current_value': 'Minimum: 1.0', 'description': \"'b' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"b\")'}, {'constraint_name': 'UniquenessConstraint(Uniqueness(List(b),None,None))', 'column_name': 'b', 'current_value': 'ApproxDistinctness: 1.0', 'description': \"'b' is unique\", 'suggesting_rule': 'UniqueIfApproximatelyUniqueRule()', 'rule_description': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint', 'code_for_constraint': '.isUnique(\"b\")'}, {'constraint_name': \"ComplianceConstraint(Compliance('c' has no negative values,c >= 0,None,List(c),None))\", 'column_name': 'c', 'current_value': 'Minimum: 5.0', 'description': \"'c' has no negative values\", 'suggesting_rule': 'NonNegativeNumbersRule()', 'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint', 'code_for_constraint': '.isNonNegative(\"c\")'}, {'constraint_name': 'CompletenessConstraint(Completeness(c,None,None))', 'column_name': 'c', 'current_value': 'Completeness: 0.6666666666666666', 'description': \"'c' has less than 87% missing values\", 'suggesting_rule': 'RetainCompletenessRule()', 'rule_description': 'If a column is incomplete in the sample, we model its completeness as a binomial variable, estimate a confidence interval and use this to define a lower bound for the completeness', 'code_for_constraint': '.hasCompleteness(\"c\", lambda x: x >= 0.13, \"It should be above 0.13!\")'}]}\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.suggestions import *\n",
    "\n",
    "suggestionResult = ConstraintSuggestionRunner(spark) \\\n",
    "             .onData(df) \\\n",
    "             .addConstraintRule(DEFAULT()) \\\n",
    "             .run()\n",
    "\n",
    "# Constraint Suggestions in JSON format\n",
    "print(suggestionResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806fdff-4423-4ea7-951a-a2dabff36b56",
   "metadata": {},
   "source": [
    "## Constraint Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c825906-1b49-49d4-93ca-b4b0231c1165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|SizeConstraint(Si...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|MinimumConstraint...|          Failure|Value: 1.0 does n...|\n",
      "|Review Check|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.66666666...|\n",
      "|Review Check|    Warning|     Warning|UniquenessConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 3) \\\n",
    "        .hasMin(\"b\", lambda x: x == 0) \\ #hasMax\n",
    "        .isComplete(\"c\")  \\\n",
    "        .isUnique(\"a\")  \\\n",
    "        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n",
    "        .isNonNegative(\"b\")) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00c2bb-714b-45a0-a23e-4fce3aaecd15",
   "metadata": {},
   "source": [
    "# Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd190b29-ef04-4c82-b0fb-b56f6b0e9c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydeequ.repository import *\n",
    "from pydeequ.analyzers import *\n",
    "\n",
    "metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'metrics.json')\n",
    "repository = FileSystemMetricsRepository(spark, metrics_file)\n",
    "key_tags = {'tag': 'pydeequ hello world'}\n",
    "resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addAnalyzer(ApproxCountDistinct('b')) \\\n",
    "    .useRepository(repository) \\\n",
    "    .saveOrAppendResult(resultKey) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a92c980e-c42b-4824-91f2-a4465c3c2f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_metrep_df = repository.load() \\\n",
    "    .before(ResultKey.current_milli_time()) \\\n",
    "    .forAnalyzers([ApproxCountDistinct('b')]) \\\n",
    "    .getSuccessMetricsAsDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48a4dd5-f21e-4761-b226-cc9e3d35e0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+-----+-------------+-------------------+\n",
      "|entity|instance|               name|value| dataset_date|                tag|\n",
      "+------+--------+-------------------+-----+-------------+-------------------+\n",
      "|Column|       b|ApproxCountDistinct|  3.0|1714572394244|pydeequ hello world|\n",
      "+------+--------+-------------------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_metrep_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d00dbb-364a-46b6-bbd3-cbbef04a6059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
