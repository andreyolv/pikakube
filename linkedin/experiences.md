Data Reliability Engineer
Clickbbus
Full-time · Current
Remote

---------------
Raízen
Full-time · 3 yrs 3 mos
Remote

Data Platform Engineer | DataOps Engineer | Data Reliability Engineer | Part 1
- Data Engineering tools such as Apache Airflow (+1000 Dags in production), Apache Spark, Apache Kafka, dbt, etc., on Kubernetes for orchestration and automation of data pipelines and distributed processing.
- Develop custom solutions to enhance platform value using programming languages like Python and Go.
- Create automation scripts for infrastructure maintenance and data tool management on Kubernetes.
- Support the data governance team with automation and monitoring to enforce quality policy gates, such as DAG policies for Airflow.
- Manage risks, deadlines, and project scope for platform initiatives using Agile methodologies such as Scrum and Kanban.
- Propose technical solutions to solve platform challenges, developing MVPs/POCs to demonstrate how new technologies and projects can drive business goals and value.
- Maintain up-to-date platform documentation that is clear, concise, and follows best practices, including architectural recommendations and ArchiMate frameworks, focusing on Data & Analytics reference architecture.
- Create and maintain a service catalog to provide visibility into existing platform assets and facilitate the design of new solutions.
- Collaborate with cross-functional teams, including data governance, data engineers, data scientists, infrastructure engineers, and solution architects, to share knowledge, promote best practices, and foster a DevOps and DataOps culture within the organization.

Data Platform Engineer | DataOps Engineer | Cloud Engineer | Part 2
- Design and implement managed cloud computing services to integrate with a Data Platform ecosystem, using tools such as Azure Synapse Analytics, Azure Purview, Azure Container Registry, Azure Kubernetes Service, Azure Database for PostgreSQL Servers, Azure Database Migration Service, Azure Cost Management, Storage Accounts, File Storage, Virtual Machines, Virtual Networks, Key Vault, Private Endpoints, Private Link, Network Security Groups, Route Tables, Private DNS Zones, Azure Databricks, Azure Machine Learning, Azure AI Services, Event Hubs, Container App, Microsoft Entra ID, Azure Policies, Microsoft Defender for Cloud, Azure Advisor, Log Analytics, Reservations, Azure Cache for Redis.
- Provision cloud resources in an automated and efficient way using Infrastructure as Code (IaC) with tools like Crossplane and Terraform.
- Azure Well-Architected Framework and Azure Naming Tool, policies, governance.

Data Platform Engineer | DataOps Engineer | DevOps Engineer | Part 3
- Utilize Kubernetes automation capabilities to automatically deploy, scale, and manage the compute, storage, and networking resources needed for data workloads.
- Design and manage CI/CD pipelines for releasing DAGs on Airflow using tools like GitHub Actions. Automating image release tagging. Standardizing and centralize GitHub Actions workflows to reuse and change management across github organization.
- Administer GitHub organization, defining branch standards, pull request workflows, rulesets, and custom properties to enforce best practices across teams.
- Develop automated repository provisioning using templates to accelerate project initialization and ensure stadarization across repositories and projects.
- Configure and maintain private GitHub Actions runners on Kubernetes to run workfloads inside VPN ensuring security.
- Automate Docker image updates in Kubernetes deployments for project teams, utilizing tools like Flux Image Update.

Data Platform Engineer | DataOps Engineer | FinOps | Part 4
- Designed and implemented Grafana dashboards to visualize and monitor resource usage in Kubernetes clusters, helping data platform users optimize their workload resources and avoid wastage by highlighting the difference between Request and Usage resources. índice de eficiencia de recursos monitoring | métrica de desperdício
- Deployed and utilized Kubecost to track and optimize Kubernetes resource spending.
- Implemented a custom Kubernetes scheduler with the MostAllocated strategy to optimize resource allocation efficiency and minimize waste.
- Adopted Kubernetes Event-Driven Autoscaler (KEDA) to dynamically scale workloads based on custom metrics, optimizing resource usage.
- Implemented of Spot Instance strategy for data solutions on Kubernetes saving costs by more than 50%.
- Developed a data pipeline solution to automate cost allocation for projects and departments using Kubecost and Azure Cost Management, with reporting integrated into Power BI.
- Plan cloud reservations for VM AKS, azure database for postgresql, storage, azure files, databricks, etc

Data Platform Engineer | DataOps Engineer | Network | Part 5
- Understand fundamental network concepts such as Virtual Networks, Subnets, DNS, VPN, Firewalls, Peering, Load Balancer, Private Endpoints, Private Links, Network Security Groups ... in order to collaborate with IT teams and support data platform users with network infrastructure-related inquiries.
- Design and configure subnets for Azure resources, ensuring appropriate CIDR block sizing for efficient network allocation.
- Plan, configure, and optimize network policies, namespaces, and ingress/egress rules to support Kubernetes clusters. Design and refine Kubernetes network policies to balance security and performance, preventing unnecessary traffic between namespaces or pods.
- Configure ingress controllers and manage external access to Kubernetes services, including TLS/SSL termination.
- Expose applications securely using Nginx or Pomerium, with Pomerium integrated into Entra ID access groups to manage traffic and enforce access controls based on user roles and permissions.
- Deploy cloud resources while following cloud security best practices, including configuring Private Endpoints and Private Links to ensure secure access to resources exclusively through the internal network.

Data Platform Engineer | DataOps Engineer | Observability | Part 6
- Collecting and analyzing infrastructure metrics to identify anomalous behaviors and potential issues in the data platform infrastructure with tools such as Prometheus.
- Collecting logs using tools like Fluent Bit and storing them, as well as maintaining log history, in platforms such as Elastic or Loki.
- Creating informative and actionable dashboards, with relevant metrics to identify potential infrastructure or application issues.
- Implemented and managed alerts and notifications for infrastructure on Kubernetes and critical applications, utilizing tools such as Alertmanager to ensure proactive issue detection and response.
- Responding to incidents in real-time, using observability tools to diagnose and fix issues as quickly as possible.
- Monitor infrastructure aspects of Kubernetes, such as volume space usage, VM scaling, frequent pod restarts, and misconfigured resources, to proactively understand cluster behavior and make adjustments before potential issues arise.
- Continuously improving the observability of infrastructure and data applications on Kubernetes, implementing alerts to proactively address potential issues using Prometheus and Grafana.
- Implement custom metric exporters or instrument applications as needed, ensuring the ability to monitor and gain insights into specific aspects of the system or application performance.
- Create mock workloads in data applications to serve as a baseline for active infrastructure performance and health monitoring, such as Airflow DAGs for checkups or Kafka producers and consumers.
- Define guidelines for monitoring and alerting to assist data platform users in monitoring data pipelines, encouraging and facilitating the use of monitoring as a important component of data solutions.

Data Platform Engineer | DataOps Engineer | Platform Engineer | Part 7
- Leverage Kubernetes automation to deploy, scale, and manage the compute, storage, and networking resources required for data workloads.
- Implement GitOps tools, such as Flux or ArgoCD, to automate kuberentes deploy, improve collaboration through declarative version-controlled infrastructure on Kuberenetes, and provide consistency and visibility.
- Utilize Flux multi-repository support for efficient management of multiple repositories, decentralizing the use of Kubernetes resources for project teams with GitOps, granting them autonomy while restricting cluster access for security purposes.
- Architect, develop, and maintain data infrastructure on Kubernetes and cloud platforms, automating processes and integrating tools to enhance the productivity of data engineers.
- Develop and maintain a development environment with Airflow, Jupyter, and VSCode on Kubernetes, templated by a custom Helm chart, to support dozens of users such as data engineers, analytics engineers, and data scientists, facilitating and accelerating the development of data pipelines and data exploration.
- Implement an Identity Provider (IDP) solution with Backstage, providing templates that allow data platform users to request custom infrastructure or platform services, automating resource provisioning and configurations with predefined and standardized templates, thereby reducing setup time.
- Use infrastructure as code (IaC) in a declarative manner to provisioning cloud computing resources on Azure, integrated with GitOps practices, with tools such as Crossplane or Terraform.
- Developed a custom Kubernetes operator for secret injection, enabling Kubernetes clusters to automatically retrieve secrets from the Senha Segura vault.
- Develop and manage a platform to create Kafka topics and users, implementing governance rules to ensure access and security policies.

Data Platform Engineer | DataOps Engineer | Security | Part 8
- Enforce cloud security best practices by following Azure Policies, such as deploying private endpoints within the internal network to ensure secure access to resources.
- Manage Kubernetes security posture and compliance using tools like Kubescape and Kubebench to assess and enforce security best practices.
- Implement supply chain security policies on Kubernetes using Gatekeeper or Kyverno to enforce compliance and security controls.
- Automate Kubernetes secrets injection in Kubernetes clusters using External Secrets Operator to synchronize secrets from vaults like Azure Key Vault or HashiCorp Vault.
- Update certificates for internal domains in the Ingress Controller, ensuring the use of HTTPS/TLS for secure, encrypted communication in transit.
- Automate image security and vulnerability scanning with Trivy in GitHub Actions, while internalizing images for improved security.
- Integrate SSO authentication using services like GitHub or Entra ID via OAuth 2.0 or OIDC with tools such as Grafana, Backstage, Airflow, Kafka UI, and Trino.
- Implement a Secrets Backend for Apache Airflow to synchronize connections with secret vaults like Azure Key Vault or HashiCorp Vault, ensuring secure, centralized management of credentials and secrets.
- Implement and enforce Static Application Security Testing (SAST) with GitHub Advanced Security and CodeQL to identify and minimize CVEs, while establishing security guidelines and monitoring practices across the organization for continuous improvement in code security.

Data Platform Engineer | DataOps Engineer | Site Reliability Engineer | Part 9
- Deploy and manage Kubernetes clusters to orchestrate containers that run applications and data services, ensuring high availability, scalability, and resilience.
- Manage the lifecycle of data and platform engineering tools in Kubernetes, applying patches and version updates without impacting service availability. Avaliar impacto de mudanças, informando os times de engenharia dos ajustes e atenções necessárias.
- Diagnose and resolve Kubernetes-related issues, such as outages deployment, network issues, and misconfigurations, to maintain the integrity and availability of data services.
- Design, implement, and maintain backup strategies to ensure data is protected and can be restored in case of failure.
- Conduct capacity planning and scaling of infrastructure to meet growth, managing resources and demand efficiently.
- Identify and resolve infrastructure failure points that may impact the availability of infrastructure and data platform services.
- Define and monitor SLOs, SLAs, and SLIs for data platform infrastructure hosted on Kubernetes, ensuring alignment with business objectives.
- Respond to incidents effectively, and drive the resolution of critical system failures. Conduct post-mortems to capture lessons learned, identify root causes, and implement preventative measures to reduce the likelihood of future occurrences.
- Evaluate, recommend, and integrate new tools and technologies to enhance the efficiency, scalability, and reliability of the organization’s data platform operations.


Data Engineer
Apr 2022 - Jan 2023 · 10 mos

ROLES:
-Design, develop, optimize, and maintain data architecture and pipelines, integrating data with internal and external source and target systems.
-Develop MVP’s/POC’s to illustrate how new technologies and projects can help achieve business goals and value.
-Resolve data issues, troubleshoot system problems, help clients understand and analyze data.
-Share knowledge and problem solving with other members of your team.
-Follow agile development methodologies to deliver solutions and product features by following DevOps / DataOps practices.
-Build pipelines for data cleansing, transformation and aggregation to deliver business insights and reports using tools such as Apache Spark.
-Continuously improve orchestration processes and ELT/ETL services using tools such as Apache Airflow.
-Write and maintain documentation of data definitions, transformations, and processes to ensure data governance and data quality.

RESULTS:
- Data Engineering pipeline with Apache Spark, Apache Airflow and sending data to a SOAP API.
- Azure Synapse Analytics and Azure Purview CI/CD pipeline with GitHub Actions.
- Apache Airflow metrics observability with Grafana.
- Internalization, containerized application validation and documentation with Docker Compose, Redis and MongoDB.
- Real-Time streaming pipeline with Kafka on Kubernetes, with Kafka Connect Debezium source for Change Data Capture and Delta Lake sink.

---

Andrey Olv
Self-employed · 8 mos
Curitiba, Paraná, Brazil · Remote

Data Engineer
Jan 2022 - Apr 2022 · 4 mos
- Career Transition

Data Scientist
Sep 2021 - Dec 2021 · 4 mos
- Career Transition

---

Telecommunications Engineer
Abix Tecnologia Ltda · Full-time
Jan 2019 - May 2021 · 2 yrs 5 mos
Curitiba, Paraná, Brazil

ROLES:
- Theoretical study of radiofrequency coverage for the implementation of radiocommunication sites and interpretation of software results.
- Site Survey for the implementation of radio communication sites.
- Radiocommunication projects - DMR (Digital Mobile Radio) and TETRA (Terrestrial Trunked Radio).
- Electrical installations and equipments in explosive atmospheres - ABNT NBR IEC 60079/ATEX/IECEx.
- Preventive maintenance inspection and site validation - R56 Motorola.
- Grounding and Lightning protection - ABNT NBR5419.
- Performance analysis of radiofrequency systems - Anritsu spectrum analyzer.
- Radio link projects - Cambium Networks, Ubiquiti, 4RF.
- Project documentation.
- Installation, configuration, testing and analysis of new products and solutions (Proof of concept).
- Communication with partners/suppliers to answer technical questions about new solutions.
- Demonstration of innovative technical solutions showing confidence for high ticket customers with the sales team.
- Support for field installation teams and remote access to equipment configurations.
- Human exposure compliance report to electromagnetic fields according to government agency ANATEL - Resolution nº700, Act nº458, Act nº1674.

RESULTS:
- I had my first project as a Project Manager.
- Engineer responsible for the radio communication system at the Rock in Rio 2019 event.
- First project in the company of a PoC (PTT over cellular) radio solution via Wi-Fi/4G interoperable with TETRA network.
- First project in the company of a BDA (Bi-Directional Amplifier) solution to expand the coverage of the TETRA network.
- First project in the company of NLOS (Non Line Of Sight) radio link (430MHz, 2Mbits, 22Km).

LESSONS LEARNED:
- How the backstage work of a project sector, all its stages, difficulties and how small details can impact the course of projects.
- I improved a lot my communication due to contact with customers, suppliers and the sales team.

---

Telecommunications Technician
Stalker Engenharia · Full-time
Feb 2017 - Jan 2019 · 2 yrs
Curitiba, Paraná, Brazil

ROLES:
- Field leader in telecommunications site activities.
- Installation of new sites, relocation, hot swap, uninstall, configuration of telecommunications equipments.
- Installation of 3 sectors of antennas in telecommunications towers or buildings rooftops, in the azimuths, mechanical and electrical tilts according project.
- Installation of cabinet, BBU (Baseband Unit), RRU (Remote Radio Unit), AC/DC power, duplexer, baterry bank, optical fiber. 
- Mobile base stations - (GSM, WCDMA, LTE/2G, 3G, 4G).
- Radio links - SIAE, Alcatel, Huawei, Nokia.
- Photographic quality report and approval of installations.
- Work at height (NR35) and installations and services in electricity (NR10).
- Outsourced service for the biggest Brazilian telecommunications companies - Claro, TIM, Telefônica/Vivo.

RESULTS:
- The most productive team in Huawei Project, which performed activities between 20-40% faster, within the required quality standards.

LESSONS LEARNED:
Lead teams technicians in the field and deal with different types of personalities.

---

Electrical Engineer
Andrey Olv · Contract
Jul 2016 - Dec 2018 · 2 yrs 6 mos
Greater Curitiba · Hybrid

---

Trainee Electrical Engineer
Mondelēz International · Full-time
Feb 2016 - Jun 2016 · 5 mos
Curitiba, Paraná, Brazil · On-site